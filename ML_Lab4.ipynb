{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"history_visible":true,"authorship_tag":"ABX9TyOB5joZdmDiV7zbsST+lw0K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://www.analyticsvidhya.com/blog/2021/08/linear-regression-and-gradient-descent-in-pytorch/"],"metadata":{"id":"HJSVKkhmjWhR"}},{"cell_type":"code","execution_count":41,"metadata":{"id":"UtizgAoPf-Bo","executionInfo":{"status":"ok","timestamp":1672047014148,"user_tz":-330,"elapsed":375,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}}},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"code","source":["inputs = np.array([[73, 67, 43],\n","[91, 88, 64],\n","[87, 134, 58],\n","[102, 43, 37],\n","[69, 96, 70]], dtype='float32')\n","# Input (temp, rainfall, humidity)"],"metadata":{"id":"fgblXVvfgM_6","executionInfo":{"status":"ok","timestamp":1672047014555,"user_tz":-330,"elapsed":3,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["targets = np.array([[56],\n","[81],\n","[119],\n","[22],\n","[103]], dtype='float32')\n","# Target (apples)"],"metadata":{"id":"lFOZEPOlgaRi","executionInfo":{"status":"ok","timestamp":1672047015195,"user_tz":-330,"elapsed":642,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["#convert numpy  into tensor\n","inputs=torch.from_numpy(inputs);\n","targets=torch.from_numpy(targets);\n","\n","print(inputs);\n","print(targets);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRgvtVi_heC7","executionInfo":{"status":"ok","timestamp":1672047015195,"user_tz":-330,"elapsed":13,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"54fe6c41-923d-467d-e00a-b35e071470e0"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]])\n","tensor([[ 56.],\n","        [ 81.],\n","        [119.],\n","        [ 22.],\n","        [103.]])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","dataset = TensorDataset(inputs, targets)\n","\n","print(dataset[:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XB76vTIdid3y","executionInfo":{"status":"ok","timestamp":1672047015195,"user_tz":-330,"elapsed":10,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"59e5a81e-5c87-402f-c9da-070eaa5ff818"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]]), tensor([[ 56.],\n","        [ 81.],\n","        [119.],\n","        [ 22.],\n","        [103.]]))\n"]}]},{"cell_type":"markdown","source":["Using Pytorchâ€™s DataLoader class we can convert the dataset into batches of predefined batch size and create batches by picking samples from the dataset randomly."],"metadata":{"id":"YMskEzHApUtI"}},{"cell_type":"code","source":["\n","from torch.utils.data import DataLoader\n","\n","batch_size=3\n","train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"],"metadata":{"id":"c4_HtXaWk2Zh","executionInfo":{"status":"ok","timestamp":1672047015196,"user_tz":-330,"elapsed":9,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["We can access the data from DataLoader as a tuple pair containing input and corresponding targets using a for loop which enables us to load batches directly into a training loop."],"metadata":{"id":"cGJPas9OpW4w"}},{"cell_type":"code","source":["# A Batch Sample\n","for inp,target in train_loader:\n","    print(inp)\n","    print(target)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUURkiKCo-5J","executionInfo":{"status":"ok","timestamp":1672047015196,"user_tz":-330,"elapsed":9,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"a55ca633-b25d-4037-daca-7c0643a43099"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 69.,  96.,  70.],\n","        [ 73.,  67.,  43.],\n","        [ 87., 134.,  58.]])\n","tensor([[103.],\n","        [ 56.],\n","        [119.]])\n"]}]},{"cell_type":"code","source":["#torch.randn generates tensors randomly from a uniform distribution with mean 0 and standard deviation 1.\n","w = torch.randn(2, 3, requires_grad=True)\n","b = torch.randn(2, requires_grad=True)\n","print(w)\n","print(b)"],"metadata":{"id":"DBeTCmClpzAW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672047015196,"user_tz":-330,"elapsed":8,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"024b353c-4fa4-4a81-9f70-112d2a899345"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.3059,  1.0248, -1.3658],\n","        [ 1.0789,  2.0404,  0.3302]], requires_grad=True)\n","tensor([0.5640, 0.1469], requires_grad=True)\n"]}]},{"cell_type":"code","source":["# define our Linear Regression model,\n","def model(X):\n","    return X @ w.t() + b\n","    "],"metadata":{"id":"vF2VToBirsdY","executionInfo":{"status":"ok","timestamp":1672047015196,"user_tz":-330,"elapsed":7,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["\n","for x,y in train_loader:\n","    preds = model(x)\n","    print(\"Prediction is :n\",preds)\n","    print(\"Actual targets is :n\",y)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vewJQQNLtwQY","executionInfo":{"status":"ok","timestamp":1672047015197,"user_tz":-330,"elapsed":8,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"19608aeb-4036-4bcf-a8af-8ac19ed93dd4"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction is :n tensor([[ 24.4442, 293.5832],\n","        [ 85.2806, 386.5744],\n","        [ 31.1702, 299.0154]], grad_fn=<AddBackward0>)\n","Actual targets is :n tensor([[103.],\n","        [119.],\n","        [ 81.]])\n"]}]},{"cell_type":"code","source":["#def loss function/ Mean Squared Error or L2 loss. \n","def mse_loss(predictions, targets):\n","    difference = predictions - targets\n","    return torch.sum(difference * difference)/ difference.numel()"],"metadata":{"id":"SBuWh2HRwjhG","executionInfo":{"status":"ok","timestamp":1672047015197,"user_tz":-330,"elapsed":6,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["for x,y in train_loader:\n","    preds = model(x)\n","    print(\"Prediction is :n\",preds)\n","    print(\"nActual targets is :n\",y)\n","    print(\"nLoss is: \",mse_loss(preds, y))\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmlJTvA6wrYl","executionInfo":{"status":"ok","timestamp":1672047015197,"user_tz":-330,"elapsed":6,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"fb1be313-3af9-4ba8-8a83-0afd7a413daa"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction is :n tensor([[ 24.4442, 293.5832],\n","        [ 25.2963, 210.1513],\n","        [ 85.2806, 386.5744]], grad_fn=<AddBackward0>)\n","nActual targets is :n tensor([[103.],\n","        [ 22.],\n","        [119.]])\n","nLoss is:  tensor(25106.3047, grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["epochs = 100\n","for i in range(epochs):\n","    # Iterate through training dataloader\n","    for x,y in train_loader:\n","        # Generate Prediction\n","        preds = model(x)\n","        # Get the loss and perform backpropagation\n","        loss = mse_loss(preds, y)\n","        loss.backward()\n","        # Let's update the weights\n","        with torch.no_grad():\n","            w -= w.grad *1e-6\n","            b -= b.grad * 1e-6\n","            # Set the gradients to zero\n","            w.grad.zero_()\n","            b.grad.zero_()\n","            print(f\"Epoch {i}/{epochs}: Loss: {loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KhkVaUR5yDmX","executionInfo":{"status":"ok","timestamp":1672048157138,"user_tz":-330,"elapsed":427,"user":{"displayName":"CE040_ Jay_Gajera","userId":"06553143479182270893"}},"outputId":"f4d5f673-0d7a-4f3f-a4f8-a988239989ce"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/100: Loss: 517.6774291992188\n","Epoch 0/100: Loss: 43.901004791259766\n","Epoch 1/100: Loss: 517.280029296875\n","Epoch 1/100: Loss: 43.814788818359375\n","Epoch 2/100: Loss: 229.457275390625\n","Epoch 2/100: Loss: 483.3143310546875\n","Epoch 3/100: Loss: 516.6277465820312\n","Epoch 3/100: Loss: 43.26210403442383\n","Epoch 4/100: Loss: 483.5226745605469\n","Epoch 4/100: Loss: 94.67074584960938\n","Epoch 5/100: Loss: 355.5508728027344\n","Epoch 5/100: Loss: 284.083251953125\n","Epoch 6/100: Loss: 198.03871154785156\n","Epoch 6/100: Loss: 520.1531372070312\n","Epoch 7/100: Loss: 514.6643676757812\n","Epoch 7/100: Loss: 43.296783447265625\n","Epoch 8/100: Loss: 481.776611328125\n","Epoch 8/100: Loss: 94.39044189453125\n","Epoch 9/100: Loss: 79.82964324951172\n","Epoch 9/100: Loss: 695.1982421875\n","Epoch 10/100: Loss: 471.4021301269531\n","Epoch 10/100: Loss: 106.3453369140625\n","Epoch 11/100: Loss: 238.2460174560547\n","Epoch 11/100: Loss: 458.74981689453125\n","Epoch 12/100: Loss: 479.0358581542969\n","Epoch 12/100: Loss: 95.46934509277344\n","Epoch 13/100: Loss: 228.58270263671875\n","Epoch 13/100: Loss: 476.5223693847656\n","Epoch 14/100: Loss: 196.95933532714844\n","Epoch 14/100: Loss: 515.8773803710938\n","Epoch 15/100: Loss: 476.1661376953125\n","Epoch 15/100: Loss: 97.64777374267578\n","Epoch 16/100: Loss: 467.8332214355469\n","Epoch 16/100: Loss: 107.53700256347656\n","Epoch 17/100: Loss: 467.5284118652344\n","Epoch 17/100: Loss: 107.3946762084961\n","Epoch 18/100: Loss: 228.53846740722656\n","Epoch 18/100: Loss: 473.22772216796875\n","Epoch 19/100: Loss: 509.5594787597656\n","Epoch 19/100: Loss: 42.55421447753906\n","Epoch 20/100: Loss: 195.72874450683594\n","Epoch 20/100: Loss: 513.7495727539062\n","Epoch 21/100: Loss: 239.83128356933594\n","Epoch 21/100: Loss: 449.52593994140625\n","Epoch 22/100: Loss: 313.3211669921875\n","Epoch 22/100: Loss: 341.32220458984375\n","Epoch 23/100: Loss: 194.70103454589844\n","Epoch 23/100: Loss: 512.945556640625\n","Epoch 24/100: Loss: 314.4173278808594\n","Epoch 24/100: Loss: 338.0722351074219\n","Epoch 25/100: Loss: 316.5880432128906\n","Epoch 25/100: Loss: 333.9718017578125\n","Epoch 26/100: Loss: 234.808837890625\n","Epoch 26/100: Loss: 453.0109558105469\n","Epoch 27/100: Loss: 461.8529052734375\n","Epoch 27/100: Loss: 108.43172454833984\n","Epoch 28/100: Loss: 191.9146728515625\n","Epoch 28/100: Loss: 513.370849609375\n","Epoch 29/100: Loss: 192.0242919921875\n","Epoch 29/100: Loss: 512.4763793945312\n","Epoch 30/100: Loss: 82.2159423828125\n","Epoch 30/100: Loss: 676.3524780273438\n","Epoch 31/100: Loss: 228.1463165283203\n","Epoch 31/100: Loss: 463.89874267578125\n","Epoch 32/100: Loss: 238.209228515625\n","Epoch 32/100: Loss: 443.5196228027344\n","Epoch 33/100: Loss: 355.1691589355469\n","Epoch 33/100: Loss: 266.33453369140625\n","Epoch 34/100: Loss: 310.4659118652344\n","Epoch 34/100: Loss: 336.2357482910156\n","Epoch 35/100: Loss: 235.32542419433594\n","Epoch 35/100: Loss: 445.3067932128906\n","Epoch 36/100: Loss: 348.4026794433594\n","Epoch 36/100: Loss: 271.67999267578125\n","Epoch 37/100: Loss: 227.216064453125\n","Epoch 37/100: Loss: 460.52972412109375\n","Epoch 38/100: Loss: 500.4581298828125\n","Epoch 38/100: Loss: 41.711856842041016\n","Epoch 39/100: Loss: 236.93763732910156\n","Epoch 39/100: Loss: 440.0834045410156\n","Epoch 40/100: Loss: 452.7569274902344\n","Epoch 40/100: Loss: 112.439208984375\n","Epoch 41/100: Loss: 452.5210876464844\n","Epoch 41/100: Loss: 112.16666412353516\n","Epoch 42/100: Loss: 346.9991149902344\n","Epoch 42/100: Loss: 269.74603271484375\n","Epoch 43/100: Loss: 347.0594787597656\n","Epoch 43/100: Loss: 268.79742431640625\n","Epoch 44/100: Loss: 309.0015563964844\n","Epoch 44/100: Loss: 331.0856018066406\n","Epoch 45/100: Loss: 496.7350769042969\n","Epoch 45/100: Loss: 42.22188949584961\n","Epoch 46/100: Loss: 496.3607482910156\n","Epoch 46/100: Loss: 42.172576904296875\n","Epoch 47/100: Loss: 232.06182861328125\n","Epoch 47/100: Loss: 441.6639099121094\n","Epoch 48/100: Loss: 308.5065612792969\n","Epoch 48/100: Loss: 329.0523681640625\n","Epoch 49/100: Loss: 449.3391418457031\n","Epoch 49/100: Loss: 111.02615356445312\n","Epoch 50/100: Loss: 356.4769592285156\n","Epoch 50/100: Loss: 251.95028686523438\n","Epoch 51/100: Loss: 312.3099060058594\n","Epoch 51/100: Loss: 321.1667175292969\n","Epoch 52/100: Loss: 226.9407196044922\n","Epoch 52/100: Loss: 445.6566162109375\n","Epoch 53/100: Loss: 312.25262451171875\n","Epoch 53/100: Loss: 319.74664306640625\n","Epoch 54/100: Loss: 458.711669921875\n","Epoch 54/100: Loss: 95.41744995117188\n","Epoch 55/100: Loss: 81.27252960205078\n","Epoch 55/100: Loss: 659.49169921875\n","Epoch 56/100: Loss: 181.05596923828125\n","Epoch 56/100: Loss: 508.8896179199219\n","Epoch 57/100: Loss: 82.1224594116211\n","Epoch 57/100: Loss: 656.5289916992188\n","Epoch 58/100: Loss: 181.1326141357422\n","Epoch 58/100: Loss: 507.0913391113281\n","Epoch 59/100: Loss: 454.2403869628906\n","Epoch 59/100: Loss: 98.25993347167969\n","Epoch 60/100: Loss: 489.3033447265625\n","Epoch 60/100: Loss: 42.632999420166016\n","Epoch 61/100: Loss: 488.9512023925781\n","Epoch 61/100: Loss: 42.55792999267578\n","Epoch 62/100: Loss: 355.9985656738281\n","Epoch 62/100: Loss: 244.01319885253906\n","Epoch 63/100: Loss: 487.9381408691406\n","Epoch 63/100: Loss: 42.71414566040039\n","Epoch 64/100: Loss: 356.4029846191406\n","Epoch 64/100: Loss: 242.04611206054688\n","Epoch 65/100: Loss: 82.14530944824219\n","Epoch 65/100: Loss: 650.9910888671875\n","Epoch 66/100: Loss: 223.6498565673828\n","Epoch 66/100: Loss: 440.409912109375\n","Epoch 67/100: Loss: 486.153076171875\n","Epoch 67/100: Loss: 42.33659744262695\n","Epoch 68/100: Loss: 178.42796325683594\n","Epoch 68/100: Loss: 504.2565612792969\n","Epoch 69/100: Loss: 344.0072937011719\n","Epoch 69/100: Loss: 254.62347412109375\n","Epoch 70/100: Loss: 83.66785430908203\n","Epoch 70/100: Loss: 644.9788208007812\n","Epoch 71/100: Loss: 484.3991394042969\n","Epoch 71/100: Loss: 42.02416229248047\n","Epoch 72/100: Loss: 446.6119079589844\n","Epoch 72/100: Loss: 100.62762451171875\n","Epoch 73/100: Loss: 213.31915283203125\n","Epoch 73/100: Loss: 455.6321105957031\n","Epoch 74/100: Loss: 444.9938659667969\n","Epoch 74/100: Loss: 101.81051635742188\n","Epoch 75/100: Loss: 342.9881286621094\n","Epoch 75/100: Loss: 252.08627319335938\n","Epoch 76/100: Loss: 482.2743835449219\n","Epoch 76/100: Loss: 42.0665168762207\n","Epoch 77/100: Loss: 434.6238708496094\n","Epoch 77/100: Loss: 113.44178009033203\n","Epoch 78/100: Loss: 305.9471435546875\n","Epoch 78/100: Loss: 311.8016052246094\n","Epoch 79/100: Loss: 343.2400207519531\n","Epoch 79/100: Loss: 249.162841796875\n","Epoch 80/100: Loss: 220.17333984375\n","Epoch 80/100: Loss: 436.2049255371094\n","Epoch 81/100: Loss: 221.292236328125\n","Epoch 81/100: Loss: 433.786865234375\n","Epoch 82/100: Loss: 351.9551086425781\n","Epoch 82/100: Loss: 236.03916931152344\n","Epoch 83/100: Loss: 479.0629577636719\n","Epoch 83/100: Loss: 42.12172317504883\n","Epoch 84/100: Loss: 210.58155822753906\n","Epoch 84/100: Loss: 452.33447265625\n","Epoch 85/100: Loss: 302.0690002441406\n","Epoch 85/100: Loss: 312.6693115234375\n","Epoch 86/100: Loss: 84.45561981201172\n","Epoch 86/100: Loss: 633.0137939453125\n","Epoch 87/100: Loss: 351.2127380371094\n","Epoch 87/100: Loss: 233.50857543945312\n","Epoch 88/100: Loss: 219.847412109375\n","Epoch 88/100: Loss: 430.85406494140625\n","Epoch 89/100: Loss: 85.12210845947266\n","Epoch 89/100: Loss: 629.6923828125\n","Epoch 90/100: Loss: 172.3274383544922\n","Epoch 90/100: Loss: 497.8966369628906\n","Epoch 91/100: Loss: 212.1687774658203\n","Epoch 91/100: Loss: 444.6985778808594\n","Epoch 92/100: Loss: 475.5278015136719\n","Epoch 92/100: Loss: 41.01226043701172\n","Epoch 93/100: Loss: 338.916015625\n","Epoch 93/100: Loss: 245.64852905273438\n","Epoch 94/100: Loss: 338.9438171386719\n","Epoch 94/100: Loss: 244.83998107910156\n","Epoch 95/100: Loss: 86.535400390625\n","Epoch 95/100: Loss: 623.4070434570312\n","Epoch 96/100: Loss: 171.4469757080078\n","Epoch 96/100: Loss: 495.0397644042969\n","Epoch 97/100: Loss: 431.1830749511719\n","Epoch 97/100: Loss: 106.52217102050781\n","Epoch 98/100: Loss: 347.7539978027344\n","Epoch 98/100: Loss: 230.93441772460938\n","Epoch 99/100: Loss: 348.5324401855469\n","Epoch 99/100: Loss: 229.00643920898438\n"]}]}]}